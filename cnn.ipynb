{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from librosa import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Data loaded. Loading time: 532.184247970581 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "path = 'C:/Users/Fabeha fatima/Anaconda3/spede/feature'\n",
    "lst = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "  for file in files:\n",
    "      try:\n",
    "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
    "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
    "        file = int(file[7:8]) - 1 \n",
    "        arr = mfccs, file\n",
    "        lst.append(arr)\n",
    "      # If the file is not valid, skip it\n",
    "      except ValueError:\n",
    "        continue\n",
    "\n",
    "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
    "X, y = zip(*lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((13978, 40), (13978,))"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving joblib files to not load them again with the loop above\n",
    "\n",
    "import joblib\n",
    "\n",
    "X_name = 'X.joblib'\n",
    "y_name = 'y.joblib'\n",
    "save_dir = 'cnn'\n",
    "\n",
    "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
    "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "X = joblib.load('cnn/X.joblib')\n",
    "y = joblib.load('cnn/y.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descion tree \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.96       581\n           1       0.79      0.81      0.80       175\n           2       0.92      0.90      0.91       684\n           3       0.90      0.92      0.91       697\n           4       0.93      0.95      0.94       675\n           5       0.91      0.91      0.91       668\n           6       0.93      0.90      0.92       572\n           7       0.91      0.92      0.92       561\n\n    accuracy                           0.92      4613\n   macro avg       0.91      0.91      0.91      4613\nweighted avg       0.92      0.92      0.92      4613\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural networks \n",
    "\n",
    "import numpy as np\n",
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "x_traincnn.shape, x_testcnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.optimizers' has no attribute 'rmsprop'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-bfb71f7d8448>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmsprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-07\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.keras.optimizers' has no attribute 'rmsprop'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(64, 5,padding='same',\n",
    "                 input_shape=(40,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(4)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(4)))\n",
    "model.add(Conv1D(256, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=1e-07, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1d_7 (Conv1D)            (None, 40, 64)            384       \n_________________________________________________________________\nactivation_9 (Activation)    (None, 40, 64)            0         \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 40, 64)            0         \n_________________________________________________________________\nmax_pooling1d_5 (MaxPooling1 (None, 10, 64)            0         \n_________________________________________________________________\nconv1d_8 (Conv1D)            (None, 10, 128)           41088     \n_________________________________________________________________\nactivation_10 (Activation)   (None, 10, 128)           0         \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 10, 128)           0         \n_________________________________________________________________\nmax_pooling1d_6 (MaxPooling1 (None, 2, 128)            0         \n_________________________________________________________________\nconv1d_9 (Conv1D)            (None, 2, 256)            164096    \n_________________________________________________________________\nactivation_11 (Activation)   (None, 2, 256)            0         \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 2, 256)            0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 512)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 8)                 4104      \n_________________________________________________________________\nactivation_12 (Activation)   (None, 8)                 0         \n=================================================================\nTotal params: 209,672\nTrainable params: 209,672\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-d16ff747b2c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model.compile(loss='sparse_categorical_crossentropy',\n\u001b[1;32m----> 2\u001b[1;33m               \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m               metrics=['accuracy'])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3518 samples, validate on 1734 samples\n",
      "Epoch 1/200\n",
      "3518/3518 [==============================] - 5s 1ms/step - loss: 4.5226 - accuracy: 0.1589 - val_loss: 1.8157 - val_accuracy: 0.3068\n",
      "Epoch 2/200\n",
      "3518/3518 [==============================] - 2s 581us/step - loss: 2.6944 - accuracy: 0.2428 - val_loss: 1.5469 - val_accuracy: 0.4619\n",
      "Epoch 3/200\n",
      "3518/3518 [==============================] - 2s 544us/step - loss: 2.0525 - accuracy: 0.3599 - val_loss: 1.5228 - val_accuracy: 0.4435\n",
      "Epoch 4/200\n",
      "3518/3518 [==============================] - 2s 503us/step - loss: 1.7633 - accuracy: 0.4090 - val_loss: 1.3575 - val_accuracy: 0.5755\n",
      "Epoch 5/200\n",
      "3518/3518 [==============================] - 2s 551us/step - loss: 1.5793 - accuracy: 0.4602 - val_loss: 1.2796 - val_accuracy: 0.5750\n",
      "Epoch 6/200\n",
      "3518/3518 [==============================] - 2s 494us/step - loss: 1.4521 - accuracy: 0.5037 - val_loss: 1.1745 - val_accuracy: 0.6280\n",
      "Epoch 7/200\n",
      "3518/3518 [==============================] - 2s 586us/step - loss: 1.3423 - accuracy: 0.5327 - val_loss: 1.1468 - val_accuracy: 0.6148\n",
      "Epoch 8/200\n",
      "3518/3518 [==============================] - 2s 494us/step - loss: 1.2671 - accuracy: 0.5583 - val_loss: 1.0510 - val_accuracy: 0.6396\n",
      "Epoch 9/200\n",
      "3518/3518 [==============================] - 2s 516us/step - loss: 1.1987 - accuracy: 0.5875 - val_loss: 1.0653 - val_accuracy: 0.6223\n",
      "Epoch 10/200\n",
      "3518/3518 [==============================] - 2s 508us/step - loss: 1.1548 - accuracy: 0.5895 - val_loss: 0.9904 - val_accuracy: 0.6459\n",
      "Epoch 11/200\n",
      "3518/3518 [==============================] - 2s 619us/step - loss: 1.0972 - accuracy: 0.6123 - val_loss: 0.9992 - val_accuracy: 0.6448\n",
      "Epoch 12/200\n",
      "3518/3518 [==============================] - 2s 544us/step - loss: 1.0467 - accuracy: 0.6302 - val_loss: 0.9655 - val_accuracy: 0.6690\n",
      "Epoch 13/200\n",
      "3518/3518 [==============================] - 2s 526us/step - loss: 1.0128 - accuracy: 0.6410 - val_loss: 0.9125 - val_accuracy: 0.6655\n",
      "Epoch 14/200\n",
      "3518/3518 [==============================] - 2s 534us/step - loss: 0.9957 - accuracy: 0.6399 - val_loss: 0.9072 - val_accuracy: 0.6811\n",
      "Epoch 15/200\n",
      "3518/3518 [==============================] - 2s 533us/step - loss: 0.9626 - accuracy: 0.6512 - val_loss: 0.8823 - val_accuracy: 0.6834\n",
      "Epoch 16/200\n",
      "3518/3518 [==============================] - 2s 536us/step - loss: 0.9453 - accuracy: 0.6637 - val_loss: 0.8698 - val_accuracy: 0.6880\n",
      "Epoch 17/200\n",
      "3518/3518 [==============================] - 2s 536us/step - loss: 0.9293 - accuracy: 0.6552 - val_loss: 0.8804 - val_accuracy: 0.6817\n",
      "Epoch 18/200\n",
      "3518/3518 [==============================] - 2s 501us/step - loss: 0.9045 - accuracy: 0.6700 - val_loss: 0.8414 - val_accuracy: 0.6932\n",
      "Epoch 19/200\n",
      "3518/3518 [==============================] - 2s 532us/step - loss: 0.9011 - accuracy: 0.6686 - val_loss: 0.8198 - val_accuracy: 0.7030\n",
      "Epoch 20/200\n",
      "3518/3518 [==============================] - 2s 547us/step - loss: 0.8804 - accuracy: 0.6728 - val_loss: 0.8272 - val_accuracy: 0.6967\n",
      "Epoch 21/200\n",
      "3518/3518 [==============================] - 2s 457us/step - loss: 0.8571 - accuracy: 0.6796 - val_loss: 0.8120 - val_accuracy: 0.7024\n",
      "Epoch 22/200\n",
      "3518/3518 [==============================] - 2s 477us/step - loss: 0.8298 - accuracy: 0.6919 - val_loss: 0.8404 - val_accuracy: 0.6886\n",
      "Epoch 23/200\n",
      "3518/3518 [==============================] - 2s 546us/step - loss: 0.8335 - accuracy: 0.6913 - val_loss: 0.7882 - val_accuracy: 0.7249\n",
      "Epoch 24/200\n",
      "3518/3518 [==============================] - 2s 593us/step - loss: 0.8126 - accuracy: 0.6998 - val_loss: 0.8105 - val_accuracy: 0.7059\n",
      "Epoch 25/200\n",
      "3518/3518 [==============================] - 2s 494us/step - loss: 0.8137 - accuracy: 0.6967 - val_loss: 0.8061 - val_accuracy: 0.6990\n",
      "Epoch 26/200\n",
      "3518/3518 [==============================] - 2s 504us/step - loss: 0.8014 - accuracy: 0.6958 - val_loss: 0.7764 - val_accuracy: 0.7261\n",
      "Epoch 27/200\n",
      "3518/3518 [==============================] - 2s 574us/step - loss: 0.7872 - accuracy: 0.7018 - val_loss: 0.7795 - val_accuracy: 0.7128\n",
      "Epoch 28/200\n",
      "3518/3518 [==============================] - 2s 562us/step - loss: 0.7804 - accuracy: 0.7089 - val_loss: 0.7537 - val_accuracy: 0.7082\n",
      "Epoch 29/200\n",
      "3518/3518 [==============================] - 2s 580us/step - loss: 0.7783 - accuracy: 0.7132 - val_loss: 0.7342 - val_accuracy: 0.7318\n",
      "Epoch 30/200\n",
      "3518/3518 [==============================] - 2s 546us/step - loss: 0.7496 - accuracy: 0.7217 - val_loss: 0.7485 - val_accuracy: 0.7163\n",
      "Epoch 31/200\n",
      "3518/3518 [==============================] - 2s 578us/step - loss: 0.7352 - accuracy: 0.7206 - val_loss: 0.7602 - val_accuracy: 0.7168\n",
      "Epoch 32/200\n",
      "3518/3518 [==============================] - 2s 512us/step - loss: 0.7475 - accuracy: 0.7214 - val_loss: 0.7361 - val_accuracy: 0.7353\n",
      "Epoch 33/200\n",
      "3518/3518 [==============================] - 2s 561us/step - loss: 0.7223 - accuracy: 0.7314 - val_loss: 0.7423 - val_accuracy: 0.7347\n",
      "Epoch 34/200\n",
      "3518/3518 [==============================] - 2s 606us/step - loss: 0.7313 - accuracy: 0.7268 - val_loss: 0.7263 - val_accuracy: 0.7359\n",
      "Epoch 35/200\n",
      "3518/3518 [==============================] - 2s 547us/step - loss: 0.7136 - accuracy: 0.7311 - val_loss: 0.7134 - val_accuracy: 0.7399\n",
      "Epoch 36/200\n",
      "3518/3518 [==============================] - 2s 573us/step - loss: 0.6932 - accuracy: 0.7405 - val_loss: 0.7116 - val_accuracy: 0.7399\n",
      "Epoch 37/200\n",
      "3518/3518 [==============================] - 2s 545us/step - loss: 0.7059 - accuracy: 0.7356 - val_loss: 0.7040 - val_accuracy: 0.7376\n",
      "Epoch 38/200\n",
      "3518/3518 [==============================] - 2s 552us/step - loss: 0.6981 - accuracy: 0.7337 - val_loss: 0.7062 - val_accuracy: 0.7399\n",
      "Epoch 39/200\n",
      "3518/3518 [==============================] - 2s 560us/step - loss: 0.6936 - accuracy: 0.7393 - val_loss: 0.6773 - val_accuracy: 0.7514\n",
      "Epoch 40/200\n",
      "3518/3518 [==============================] - 2s 524us/step - loss: 0.6789 - accuracy: 0.7428 - val_loss: 0.7305 - val_accuracy: 0.7318\n",
      "Epoch 41/200\n",
      "3518/3518 [==============================] - 2s 539us/step - loss: 0.6729 - accuracy: 0.7521 - val_loss: 0.7131 - val_accuracy: 0.7388\n",
      "Epoch 42/200\n",
      "3518/3518 [==============================] - 2s 515us/step - loss: 0.6657 - accuracy: 0.7575 - val_loss: 0.6815 - val_accuracy: 0.7526\n",
      "Epoch 43/200\n",
      "3518/3518 [==============================] - 2s 616us/step - loss: 0.6532 - accuracy: 0.7578 - val_loss: 0.6867 - val_accuracy: 0.7439\n",
      "Epoch 44/200\n",
      "3518/3518 [==============================] - 2s 557us/step - loss: 0.6532 - accuracy: 0.7513 - val_loss: 0.7277 - val_accuracy: 0.7249\n",
      "Epoch 45/200\n",
      "3518/3518 [==============================] - 2s 538us/step - loss: 0.6338 - accuracy: 0.7621 - val_loss: 0.6626 - val_accuracy: 0.7549\n",
      "Epoch 46/200\n",
      "3518/3518 [==============================] - 2s 578us/step - loss: 0.6448 - accuracy: 0.7550 - val_loss: 0.6769 - val_accuracy: 0.7480\n",
      "Epoch 47/200\n",
      "3518/3518 [==============================] - 2s 497us/step - loss: 0.6423 - accuracy: 0.7544 - val_loss: 0.6619 - val_accuracy: 0.7647\n",
      "Epoch 48/200\n",
      "3518/3518 [==============================] - 2s 465us/step - loss: 0.6412 - accuracy: 0.7604 - val_loss: 0.6554 - val_accuracy: 0.7641\n",
      "Epoch 49/200\n",
      "3518/3518 [==============================] - 2s 667us/step - loss: 0.6295 - accuracy: 0.7621 - val_loss: 0.6672 - val_accuracy: 0.7480\n",
      "Epoch 50/200\n",
      "3518/3518 [==============================] - 2s 579us/step - loss: 0.6222 - accuracy: 0.7720 - val_loss: 0.6420 - val_accuracy: 0.7647\n",
      "Epoch 51/200\n",
      "3518/3518 [==============================] - 2s 522us/step - loss: 0.6163 - accuracy: 0.7663 - val_loss: 0.6461 - val_accuracy: 0.7561\n",
      "Epoch 52/200\n",
      "3518/3518 [==============================] - 2s 561us/step - loss: 0.6049 - accuracy: 0.7803 - val_loss: 0.6457 - val_accuracy: 0.7584\n",
      "Epoch 53/200\n",
      "3518/3518 [==============================] - 3s 755us/step - loss: 0.6097 - accuracy: 0.7715 - val_loss: 0.6353 - val_accuracy: 0.7555\n",
      "Epoch 54/200\n",
      "3518/3518 [==============================] - 2s 601us/step - loss: 0.6049 - accuracy: 0.7678 - val_loss: 0.6463 - val_accuracy: 0.7699\n",
      "Epoch 55/200\n",
      "3518/3518 [==============================] - 2s 595us/step - loss: 0.5957 - accuracy: 0.7766 - val_loss: 0.6358 - val_accuracy: 0.7607\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3518/3518 [==============================] - 2s 531us/step - loss: 0.5990 - accuracy: 0.7786 - val_loss: 0.6338 - val_accuracy: 0.7624\n",
      "Epoch 57/200\n",
      "3518/3518 [==============================] - 2s 544us/step - loss: 0.5885 - accuracy: 0.7783 - val_loss: 0.6257 - val_accuracy: 0.7653\n",
      "Epoch 58/200\n",
      "3518/3518 [==============================] - 2s 483us/step - loss: 0.5791 - accuracy: 0.7882 - val_loss: 0.6176 - val_accuracy: 0.7699\n",
      "Epoch 59/200\n",
      "3518/3518 [==============================] - 2s 516us/step - loss: 0.5750 - accuracy: 0.7862 - val_loss: 0.6185 - val_accuracy: 0.7682\n",
      "Epoch 60/200\n",
      "3518/3518 [==============================] - 2s 504us/step - loss: 0.5813 - accuracy: 0.7806 - val_loss: 0.6297 - val_accuracy: 0.7641\n",
      "Epoch 61/200\n",
      "3518/3518 [==============================] - 2s 481us/step - loss: 0.5662 - accuracy: 0.7885 - val_loss: 0.6196 - val_accuracy: 0.7722\n",
      "Epoch 62/200\n",
      "3518/3518 [==============================] - 2s 483us/step - loss: 0.5777 - accuracy: 0.7845 - val_loss: 0.6211 - val_accuracy: 0.7699\n",
      "Epoch 63/200\n",
      "3518/3518 [==============================] - 2s 479us/step - loss: 0.5706 - accuracy: 0.7902 - val_loss: 0.6146 - val_accuracy: 0.7699\n",
      "Epoch 64/200\n",
      "3518/3518 [==============================] - 2s 455us/step - loss: 0.5654 - accuracy: 0.7970 - val_loss: 0.6043 - val_accuracy: 0.7716\n",
      "Epoch 65/200\n",
      "3518/3518 [==============================] - 2s 553us/step - loss: 0.5597 - accuracy: 0.7897 - val_loss: 0.5947 - val_accuracy: 0.7809\n",
      "Epoch 66/200\n",
      "3518/3518 [==============================] - 2s 536us/step - loss: 0.5475 - accuracy: 0.7953 - val_loss: 0.6051 - val_accuracy: 0.7820\n",
      "Epoch 67/200\n",
      "3518/3518 [==============================] - 2s 533us/step - loss: 0.5532 - accuracy: 0.7953 - val_loss: 0.5974 - val_accuracy: 0.7820\n",
      "Epoch 68/200\n",
      "3518/3518 [==============================] - 2s 535us/step - loss: 0.5529 - accuracy: 0.7933 - val_loss: 0.6017 - val_accuracy: 0.7716\n",
      "Epoch 69/200\n",
      "3518/3518 [==============================] - 2s 550us/step - loss: 0.5545 - accuracy: 0.7948 - val_loss: 0.6015 - val_accuracy: 0.7768\n",
      "Epoch 70/200\n",
      "3518/3518 [==============================] - 2s 512us/step - loss: 0.5460 - accuracy: 0.7970 - val_loss: 0.5896 - val_accuracy: 0.7814\n",
      "Epoch 71/200\n",
      "3518/3518 [==============================] - 2s 514us/step - loss: 0.5335 - accuracy: 0.8010 - val_loss: 0.6294 - val_accuracy: 0.7745\n",
      "Epoch 72/200\n",
      "3518/3518 [==============================] - 2s 510us/step - loss: 0.5430 - accuracy: 0.8013 - val_loss: 0.5960 - val_accuracy: 0.7803\n",
      "Epoch 73/200\n",
      "3518/3518 [==============================] - 2s 557us/step - loss: 0.5397 - accuracy: 0.8067 - val_loss: 0.5946 - val_accuracy: 0.7809\n",
      "Epoch 74/200\n",
      "3518/3518 [==============================] - 2s 490us/step - loss: 0.5299 - accuracy: 0.8022 - val_loss: 0.5840 - val_accuracy: 0.7820\n",
      "Epoch 75/200\n",
      "3518/3518 [==============================] - 2s 496us/step - loss: 0.5199 - accuracy: 0.8147 - val_loss: 0.5850 - val_accuracy: 0.7832\n",
      "Epoch 76/200\n",
      "3518/3518 [==============================] - 2s 465us/step - loss: 0.5312 - accuracy: 0.8047 - val_loss: 0.5726 - val_accuracy: 0.7941\n",
      "Epoch 77/200\n",
      "3518/3518 [==============================] - 2s 516us/step - loss: 0.5133 - accuracy: 0.8070 - val_loss: 0.5684 - val_accuracy: 0.7895\n",
      "Epoch 78/200\n",
      "3518/3518 [==============================] - 2s 543us/step - loss: 0.5140 - accuracy: 0.8098 - val_loss: 0.5760 - val_accuracy: 0.7895\n",
      "Epoch 79/200\n",
      "3518/3518 [==============================] - 2s 539us/step - loss: 0.5120 - accuracy: 0.8087 - val_loss: 0.5857 - val_accuracy: 0.7762\n",
      "Epoch 80/200\n",
      "3518/3518 [==============================] - 2s 545us/step - loss: 0.5087 - accuracy: 0.8147 - val_loss: 0.5800 - val_accuracy: 0.7814\n",
      "Epoch 81/200\n",
      "3518/3518 [==============================] - 2s 503us/step - loss: 0.5035 - accuracy: 0.8147 - val_loss: 0.5585 - val_accuracy: 0.7930\n",
      "Epoch 82/200\n",
      "3518/3518 [==============================] - 2s 535us/step - loss: 0.5086 - accuracy: 0.8110 - val_loss: 0.5778 - val_accuracy: 0.7930\n",
      "Epoch 83/200\n",
      "3518/3518 [==============================] - 2s 522us/step - loss: 0.5067 - accuracy: 0.8135 - val_loss: 0.5621 - val_accuracy: 0.7878\n",
      "Epoch 84/200\n",
      "3518/3518 [==============================] - 2s 523us/step - loss: 0.4966 - accuracy: 0.8215 - val_loss: 0.5681 - val_accuracy: 0.7935\n",
      "Epoch 85/200\n",
      "3518/3518 [==============================] - 2s 496us/step - loss: 0.4894 - accuracy: 0.8204 - val_loss: 0.5560 - val_accuracy: 0.7907\n",
      "Epoch 86/200\n",
      "3518/3518 [==============================] - 2s 482us/step - loss: 0.4968 - accuracy: 0.8195 - val_loss: 0.5642 - val_accuracy: 0.7912\n",
      "Epoch 87/200\n",
      "3518/3518 [==============================] - 2s 589us/step - loss: 0.4936 - accuracy: 0.8098 - val_loss: 0.5747 - val_accuracy: 0.7889\n",
      "Epoch 88/200\n",
      "3518/3518 [==============================] - 2s 582us/step - loss: 0.4925 - accuracy: 0.8118 - val_loss: 0.5597 - val_accuracy: 0.7970\n",
      "Epoch 89/200\n",
      "3518/3518 [==============================] - 2s 555us/step - loss: 0.4800 - accuracy: 0.8218 - val_loss: 0.5738 - val_accuracy: 0.7878\n",
      "Epoch 90/200\n",
      "3518/3518 [==============================] - 2s 559us/step - loss: 0.4891 - accuracy: 0.8246 - val_loss: 0.5395 - val_accuracy: 0.8062\n",
      "Epoch 91/200\n",
      "3518/3518 [==============================] - 2s 540us/step - loss: 0.4729 - accuracy: 0.8314 - val_loss: 0.5466 - val_accuracy: 0.7964\n",
      "Epoch 92/200\n",
      "3518/3518 [==============================] - 2s 536us/step - loss: 0.4639 - accuracy: 0.8337 - val_loss: 0.5485 - val_accuracy: 0.7976\n",
      "Epoch 93/200\n",
      "3518/3518 [==============================] - 2s 500us/step - loss: 0.4630 - accuracy: 0.8323 - val_loss: 0.5620 - val_accuracy: 0.7958\n",
      "Epoch 94/200\n",
      "3518/3518 [==============================] - 2s 460us/step - loss: 0.4645 - accuracy: 0.8306 - val_loss: 0.5395 - val_accuracy: 0.8039\n",
      "Epoch 95/200\n",
      "3518/3518 [==============================] - 2s 515us/step - loss: 0.4605 - accuracy: 0.8223 - val_loss: 0.5453 - val_accuracy: 0.7941\n",
      "Epoch 96/200\n",
      "3518/3518 [==============================] - 2s 521us/step - loss: 0.4651 - accuracy: 0.8363 - val_loss: 0.5499 - val_accuracy: 0.7953\n",
      "Epoch 97/200\n",
      "3518/3518 [==============================] - 2s 486us/step - loss: 0.4608 - accuracy: 0.8258 - val_loss: 0.5546 - val_accuracy: 0.7958\n",
      "Epoch 98/200\n",
      "3518/3518 [==============================] - 2s 483us/step - loss: 0.4582 - accuracy: 0.8272 - val_loss: 0.5627 - val_accuracy: 0.7901\n",
      "Epoch 99/200\n",
      "3518/3518 [==============================] - 2s 553us/step - loss: 0.4612 - accuracy: 0.8312 - val_loss: 0.5448 - val_accuracy: 0.8085\n",
      "Epoch 100/200\n",
      "3518/3518 [==============================] - 2s 551us/step - loss: 0.4458 - accuracy: 0.8383 - val_loss: 0.5340 - val_accuracy: 0.8033\n",
      "Epoch 101/200\n",
      "3518/3518 [==============================] - 2s 562us/step - loss: 0.4481 - accuracy: 0.8348 - val_loss: 0.5362 - val_accuracy: 0.8074\n",
      "Epoch 102/200\n",
      "3518/3518 [==============================] - 2s 469us/step - loss: 0.4545 - accuracy: 0.8309 - val_loss: 0.5308 - val_accuracy: 0.8074\n",
      "Epoch 103/200\n",
      "3518/3518 [==============================] - 2s 498us/step - loss: 0.4420 - accuracy: 0.8380 - val_loss: 0.5449 - val_accuracy: 0.8016\n",
      "Epoch 104/200\n",
      "3518/3518 [==============================] - 2s 524us/step - loss: 0.4359 - accuracy: 0.8394 - val_loss: 0.5551 - val_accuracy: 0.8010\n",
      "Epoch 105/200\n",
      "3518/3518 [==============================] - 2s 533us/step - loss: 0.4310 - accuracy: 0.8354 - val_loss: 0.5329 - val_accuracy: 0.8062\n",
      "Epoch 106/200\n",
      "3518/3518 [==============================] - 2s 487us/step - loss: 0.4278 - accuracy: 0.8394 - val_loss: 0.5337 - val_accuracy: 0.8108\n",
      "Epoch 107/200\n",
      "3518/3518 [==============================] - 2s 505us/step - loss: 0.4359 - accuracy: 0.8403 - val_loss: 0.5309 - val_accuracy: 0.8051\n",
      "Epoch 108/200\n",
      "3518/3518 [==============================] - 2s 496us/step - loss: 0.4295 - accuracy: 0.8434 - val_loss: 0.5344 - val_accuracy: 0.8062\n",
      "Epoch 109/200\n",
      "3518/3518 [==============================] - 2s 503us/step - loss: 0.4319 - accuracy: 0.8425 - val_loss: 0.5341 - val_accuracy: 0.8022\n",
      "Epoch 110/200\n",
      "3518/3518 [==============================] - 2s 480us/step - loss: 0.4227 - accuracy: 0.8491 - val_loss: 0.5215 - val_accuracy: 0.8120\n",
      "Epoch 111/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3518/3518 [==============================] - 2s 504us/step - loss: 0.4265 - accuracy: 0.8425 - val_loss: 0.5217 - val_accuracy: 0.8137\n",
      "Epoch 112/200\n",
      "3518/3518 [==============================] - 2s 591us/step - loss: 0.4031 - accuracy: 0.8516 - val_loss: 0.5125 - val_accuracy: 0.8137\n",
      "Epoch 113/200\n",
      "3518/3518 [==============================] - 2s 520us/step - loss: 0.4134 - accuracy: 0.8474 - val_loss: 0.5166 - val_accuracy: 0.8080\n",
      "Epoch 114/200\n",
      "3518/3518 [==============================] - 2s 511us/step - loss: 0.4147 - accuracy: 0.8414 - val_loss: 0.5194 - val_accuracy: 0.8062\n",
      "Epoch 115/200\n",
      "3518/3518 [==============================] - 2s 488us/step - loss: 0.4144 - accuracy: 0.8488 - val_loss: 0.5274 - val_accuracy: 0.8126\n",
      "Epoch 116/200\n",
      "3518/3518 [==============================] - 2s 480us/step - loss: 0.4041 - accuracy: 0.8493 - val_loss: 0.5217 - val_accuracy: 0.8074\n",
      "Epoch 117/200\n",
      "3518/3518 [==============================] - 2s 546us/step - loss: 0.4124 - accuracy: 0.8493 - val_loss: 0.5308 - val_accuracy: 0.8057\n",
      "Epoch 118/200\n",
      "3518/3518 [==============================] - 2s 502us/step - loss: 0.4214 - accuracy: 0.8405 - val_loss: 0.5140 - val_accuracy: 0.8137\n",
      "Epoch 119/200\n",
      "3518/3518 [==============================] - 2s 486us/step - loss: 0.4012 - accuracy: 0.8482 - val_loss: 0.5132 - val_accuracy: 0.8126\n",
      "Epoch 120/200\n",
      "3518/3518 [==============================] - 2s 501us/step - loss: 0.3958 - accuracy: 0.8556 - val_loss: 0.5205 - val_accuracy: 0.8074\n",
      "Epoch 121/200\n",
      "3518/3518 [==============================] - 2s 488us/step - loss: 0.3976 - accuracy: 0.8565 - val_loss: 0.5192 - val_accuracy: 0.8091\n",
      "Epoch 122/200\n",
      "3518/3518 [==============================] - 2s 529us/step - loss: 0.4011 - accuracy: 0.8513 - val_loss: 0.5136 - val_accuracy: 0.8131\n",
      "Epoch 123/200\n",
      "3518/3518 [==============================] - 2s 626us/step - loss: 0.3984 - accuracy: 0.8533 - val_loss: 0.5061 - val_accuracy: 0.8131\n",
      "Epoch 124/200\n",
      "3518/3518 [==============================] - 2s 606us/step - loss: 0.3962 - accuracy: 0.8573 - val_loss: 0.5119 - val_accuracy: 0.8120\n",
      "Epoch 125/200\n",
      "3518/3518 [==============================] - 2s 588us/step - loss: 0.4017 - accuracy: 0.8536 - val_loss: 0.5018 - val_accuracy: 0.8206\n",
      "Epoch 126/200\n",
      "3518/3518 [==============================] - 2s 556us/step - loss: 0.3881 - accuracy: 0.8576 - val_loss: 0.5056 - val_accuracy: 0.8189\n",
      "Epoch 127/200\n",
      "3518/3518 [==============================] - 2s 531us/step - loss: 0.3884 - accuracy: 0.8621 - val_loss: 0.5275 - val_accuracy: 0.8114\n",
      "Epoch 128/200\n",
      "3518/3518 [==============================] - 2s 596us/step - loss: 0.3759 - accuracy: 0.8658 - val_loss: 0.5031 - val_accuracy: 0.8143\n",
      "Epoch 129/200\n",
      "3518/3518 [==============================] - 2s 529us/step - loss: 0.3802 - accuracy: 0.8613 - val_loss: 0.5084 - val_accuracy: 0.8166\n",
      "Epoch 130/200\n",
      "3518/3518 [==============================] - 2s 481us/step - loss: 0.3832 - accuracy: 0.8579 - val_loss: 0.5042 - val_accuracy: 0.8172\n",
      "Epoch 131/200\n",
      "3518/3518 [==============================] - 2s 456us/step - loss: 0.3769 - accuracy: 0.8604 - val_loss: 0.5040 - val_accuracy: 0.8189\n",
      "Epoch 132/200\n",
      "3518/3518 [==============================] - 2s 470us/step - loss: 0.3829 - accuracy: 0.8601 - val_loss: 0.4995 - val_accuracy: 0.8166\n",
      "Epoch 133/200\n",
      "3518/3518 [==============================] - 2s 549us/step - loss: 0.3690 - accuracy: 0.8601 - val_loss: 0.4950 - val_accuracy: 0.8230\n",
      "Epoch 134/200\n",
      "3518/3518 [==============================] - 2s 523us/step - loss: 0.3752 - accuracy: 0.8573 - val_loss: 0.4992 - val_accuracy: 0.8230\n",
      "Epoch 135/200\n",
      "3518/3518 [==============================] - 2s 515us/step - loss: 0.3802 - accuracy: 0.8587 - val_loss: 0.4959 - val_accuracy: 0.8253\n",
      "Epoch 136/200\n",
      "3518/3518 [==============================] - 2s 528us/step - loss: 0.3713 - accuracy: 0.8619 - val_loss: 0.4956 - val_accuracy: 0.8201\n",
      "Epoch 137/200\n",
      "3518/3518 [==============================] - 2s 523us/step - loss: 0.3671 - accuracy: 0.8636 - val_loss: 0.4946 - val_accuracy: 0.8189\n",
      "Epoch 138/200\n",
      "3518/3518 [==============================] - 2s 489us/step - loss: 0.3746 - accuracy: 0.8638 - val_loss: 0.4885 - val_accuracy: 0.8253\n",
      "Epoch 139/200\n",
      "3518/3518 [==============================] - 2s 522us/step - loss: 0.3608 - accuracy: 0.8650 - val_loss: 0.5026 - val_accuracy: 0.8114\n",
      "Epoch 140/200\n",
      "3518/3518 [==============================] - 2s 497us/step - loss: 0.3576 - accuracy: 0.8644 - val_loss: 0.4970 - val_accuracy: 0.8183\n",
      "Epoch 141/200\n",
      "3518/3518 [==============================] - 2s 486us/step - loss: 0.3654 - accuracy: 0.8658 - val_loss: 0.4951 - val_accuracy: 0.8247\n",
      "Epoch 142/200\n",
      "3518/3518 [==============================] - 2s 481us/step - loss: 0.3681 - accuracy: 0.8644 - val_loss: 0.4969 - val_accuracy: 0.8155\n",
      "Epoch 143/200\n",
      "3518/3518 [==============================] - 2s 483us/step - loss: 0.3540 - accuracy: 0.8707 - val_loss: 0.4962 - val_accuracy: 0.8230\n",
      "Epoch 144/200\n",
      "3518/3518 [==============================] - 2s 481us/step - loss: 0.3501 - accuracy: 0.8681 - val_loss: 0.4918 - val_accuracy: 0.8235\n",
      "Epoch 145/200\n",
      "3518/3518 [==============================] - 2s 504us/step - loss: 0.3542 - accuracy: 0.8712 - val_loss: 0.5014 - val_accuracy: 0.8149\n",
      "Epoch 146/200\n",
      "3518/3518 [==============================] - 2s 500us/step - loss: 0.3500 - accuracy: 0.8764 - val_loss: 0.4939 - val_accuracy: 0.8293\n",
      "Epoch 147/200\n",
      "3518/3518 [==============================] - 2s 513us/step - loss: 0.3564 - accuracy: 0.8687 - val_loss: 0.4841 - val_accuracy: 0.8241\n",
      "Epoch 148/200\n",
      "3518/3518 [==============================] - 2s 505us/step - loss: 0.3481 - accuracy: 0.8735 - val_loss: 0.4818 - val_accuracy: 0.8241\n",
      "Epoch 149/200\n",
      "3518/3518 [==============================] - 2s 516us/step - loss: 0.3450 - accuracy: 0.8744 - val_loss: 0.4916 - val_accuracy: 0.8201\n",
      "Epoch 150/200\n",
      "3518/3518 [==============================] - 2s 507us/step - loss: 0.3484 - accuracy: 0.8670 - val_loss: 0.4929 - val_accuracy: 0.8247\n",
      "Epoch 151/200\n",
      "3518/3518 [==============================] - 2s 515us/step - loss: 0.3556 - accuracy: 0.8746 - val_loss: 0.4929 - val_accuracy: 0.8201\n",
      "Epoch 152/200\n",
      "3518/3518 [==============================] - 2s 508us/step - loss: 0.3473 - accuracy: 0.8738 - val_loss: 0.4919 - val_accuracy: 0.8224\n",
      "Epoch 153/200\n",
      "3518/3518 [==============================] - 2s 483us/step - loss: 0.3363 - accuracy: 0.8789 - val_loss: 0.4952 - val_accuracy: 0.8183\n",
      "Epoch 154/200\n",
      "3518/3518 [==============================] - 2s 534us/step - loss: 0.3292 - accuracy: 0.8800 - val_loss: 0.4819 - val_accuracy: 0.8281\n",
      "Epoch 155/200\n",
      "3518/3518 [==============================] - 2s 495us/step - loss: 0.3414 - accuracy: 0.8795 - val_loss: 0.5016 - val_accuracy: 0.8178\n",
      "Epoch 156/200\n",
      "3518/3518 [==============================] - 2s 501us/step - loss: 0.3328 - accuracy: 0.8761 - val_loss: 0.4906 - val_accuracy: 0.8264\n",
      "Epoch 157/200\n",
      "3518/3518 [==============================] - 2s 465us/step - loss: 0.3273 - accuracy: 0.8789 - val_loss: 0.4917 - val_accuracy: 0.8276\n",
      "Epoch 158/200\n",
      "3518/3518 [==============================] - 2s 529us/step - loss: 0.3336 - accuracy: 0.8778 - val_loss: 0.4864 - val_accuracy: 0.8247\n",
      "Epoch 159/200\n",
      "3518/3518 [==============================] - 2s 536us/step - loss: 0.3294 - accuracy: 0.8764 - val_loss: 0.4843 - val_accuracy: 0.8264\n",
      "Epoch 160/200\n",
      "3518/3518 [==============================] - 2s 526us/step - loss: 0.3238 - accuracy: 0.8818 - val_loss: 0.4999 - val_accuracy: 0.8166\n",
      "Epoch 161/200\n",
      "3518/3518 [==============================] - 2s 541us/step - loss: 0.3204 - accuracy: 0.8800 - val_loss: 0.4922 - val_accuracy: 0.8201\n",
      "Epoch 162/200\n",
      "3518/3518 [==============================] - 2s 509us/step - loss: 0.3116 - accuracy: 0.8840 - val_loss: 0.4866 - val_accuracy: 0.8345\n",
      "Epoch 163/200\n",
      "3518/3518 [==============================] - 2s 554us/step - loss: 0.3226 - accuracy: 0.8840 - val_loss: 0.4929 - val_accuracy: 0.8258\n",
      "Epoch 164/200\n",
      "3518/3518 [==============================] - 2s 562us/step - loss: 0.3295 - accuracy: 0.8775 - val_loss: 0.4826 - val_accuracy: 0.8253\n",
      "Epoch 165/200\n",
      "3518/3518 [==============================] - 2s 560us/step - loss: 0.3090 - accuracy: 0.8840 - val_loss: 0.4864 - val_accuracy: 0.8276\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3518/3518 [==============================] - 2s 545us/step - loss: 0.3165 - accuracy: 0.8869 - val_loss: 0.4947 - val_accuracy: 0.8235\n",
      "Epoch 167/200\n",
      "3518/3518 [==============================] - 2s 517us/step - loss: 0.3161 - accuracy: 0.8872 - val_loss: 0.4884 - val_accuracy: 0.8293\n",
      "Epoch 168/200\n",
      "3518/3518 [==============================] - 2s 506us/step - loss: 0.3166 - accuracy: 0.8852 - val_loss: 0.4892 - val_accuracy: 0.8230\n",
      "Epoch 169/200\n",
      "3518/3518 [==============================] - 2s 483us/step - loss: 0.3190 - accuracy: 0.8852 - val_loss: 0.4905 - val_accuracy: 0.8230\n",
      "Epoch 170/200\n",
      "3518/3518 [==============================] - 2s 483us/step - loss: 0.3084 - accuracy: 0.8889 - val_loss: 0.4765 - val_accuracy: 0.8345\n",
      "Epoch 171/200\n",
      "3518/3518 [==============================] - 2s 489us/step - loss: 0.2979 - accuracy: 0.8900 - val_loss: 0.4839 - val_accuracy: 0.8247\n",
      "Epoch 172/200\n",
      "3518/3518 [==============================] - 2s 493us/step - loss: 0.3060 - accuracy: 0.8883 - val_loss: 0.4974 - val_accuracy: 0.8247\n",
      "Epoch 173/200\n",
      "3518/3518 [==============================] - 2s 487us/step - loss: 0.2991 - accuracy: 0.8917 - val_loss: 0.4799 - val_accuracy: 0.8264\n",
      "Epoch 174/200\n",
      "3518/3518 [==============================] - 2s 489us/step - loss: 0.2969 - accuracy: 0.8957 - val_loss: 0.5207 - val_accuracy: 0.8160\n",
      "Epoch 175/200\n",
      "3518/3518 [==============================] - 2s 498us/step - loss: 0.3129 - accuracy: 0.8843 - val_loss: 0.4630 - val_accuracy: 0.8333\n",
      "Epoch 176/200\n",
      "3518/3518 [==============================] - 2s 521us/step - loss: 0.2954 - accuracy: 0.8920 - val_loss: 0.4750 - val_accuracy: 0.8362\n",
      "Epoch 177/200\n",
      "3518/3518 [==============================] - 2s 526us/step - loss: 0.3017 - accuracy: 0.8891 - val_loss: 0.4861 - val_accuracy: 0.8287\n",
      "Epoch 178/200\n",
      "3518/3518 [==============================] - 2s 526us/step - loss: 0.3126 - accuracy: 0.8894 - val_loss: 0.4761 - val_accuracy: 0.8258\n",
      "Epoch 179/200\n",
      "3518/3518 [==============================] - 2s 526us/step - loss: 0.2924 - accuracy: 0.8957 - val_loss: 0.4738 - val_accuracy: 0.8293\n",
      "Epoch 180/200\n",
      "3518/3518 [==============================] - 2s 526us/step - loss: 0.2947 - accuracy: 0.8886 - val_loss: 0.4709 - val_accuracy: 0.8333\n",
      "Epoch 181/200\n",
      "3518/3518 [==============================] - 2s 526us/step - loss: 0.2863 - accuracy: 0.8951 - val_loss: 0.4839 - val_accuracy: 0.8293\n",
      "Epoch 182/200\n",
      "3518/3518 [==============================] - 2s 525us/step - loss: 0.2834 - accuracy: 0.9025 - val_loss: 0.4601 - val_accuracy: 0.8339\n",
      "Epoch 183/200\n",
      "3518/3518 [==============================] - 2s 522us/step - loss: 0.2957 - accuracy: 0.8943 - val_loss: 0.4622 - val_accuracy: 0.8374\n",
      "Epoch 184/200\n",
      "3518/3518 [==============================] - 2s 526us/step - loss: 0.2817 - accuracy: 0.8980 - val_loss: 0.4731 - val_accuracy: 0.8299\n",
      "Epoch 185/200\n",
      "3518/3518 [==============================] - 2s 489us/step - loss: 0.2918 - accuracy: 0.8934 - val_loss: 0.4721 - val_accuracy: 0.8304\n",
      "Epoch 186/200\n",
      "3518/3518 [==============================] - 2s 491us/step - loss: 0.2798 - accuracy: 0.8943 - val_loss: 0.5020 - val_accuracy: 0.8235\n",
      "Epoch 187/200\n",
      "3518/3518 [==============================] - 2s 483us/step - loss: 0.2791 - accuracy: 0.9034 - val_loss: 0.4780 - val_accuracy: 0.8316\n",
      "Epoch 188/200\n",
      "3518/3518 [==============================] - 2s 488us/step - loss: 0.2873 - accuracy: 0.8965 - val_loss: 0.4986 - val_accuracy: 0.8293\n",
      "Epoch 189/200\n",
      "3518/3518 [==============================] - 2s 497us/step - loss: 0.2761 - accuracy: 0.9016 - val_loss: 0.4513 - val_accuracy: 0.8403\n",
      "Epoch 190/200\n",
      "3518/3518 [==============================] - 2s 483us/step - loss: 0.2693 - accuracy: 0.9005 - val_loss: 0.4581 - val_accuracy: 0.8443\n",
      "Epoch 191/200\n",
      "3518/3518 [==============================] - 2s 540us/step - loss: 0.2751 - accuracy: 0.8957 - val_loss: 0.4762 - val_accuracy: 0.8293\n",
      "Epoch 192/200\n",
      "3518/3518 [==============================] - 2s 492us/step - loss: 0.2719 - accuracy: 0.9011 - val_loss: 0.4628 - val_accuracy: 0.8397\n",
      "Epoch 193/200\n",
      "3518/3518 [==============================] - 2s 493us/step - loss: 0.2740 - accuracy: 0.9005 - val_loss: 0.4639 - val_accuracy: 0.8368\n",
      "Epoch 194/200\n",
      "3518/3518 [==============================] - 2s 487us/step - loss: 0.2733 - accuracy: 0.9022 - val_loss: 0.4907 - val_accuracy: 0.8299\n",
      "Epoch 195/200\n",
      "3518/3518 [==============================] - 2s 493us/step - loss: 0.2660 - accuracy: 0.9096 - val_loss: 0.4728 - val_accuracy: 0.8304\n",
      "Epoch 196/200\n",
      "3518/3518 [==============================] - 2s 530us/step - loss: 0.2727 - accuracy: 0.8991 - val_loss: 0.4687 - val_accuracy: 0.8408\n",
      "Epoch 197/200\n",
      "3518/3518 [==============================] - 2s 522us/step - loss: 0.2616 - accuracy: 0.9028 - val_loss: 0.4902 - val_accuracy: 0.8345\n",
      "Epoch 198/200\n",
      "3518/3518 [==============================] - 2s 492us/step - loss: 0.2681 - accuracy: 0.9039 - val_loss: 0.4773 - val_accuracy: 0.8391\n",
      "Epoch 199/200\n",
      "3518/3518 [==============================] - 2s 517us/step - loss: 0.2605 - accuracy: 0.9051 - val_loss: 0.4767 - val_accuracy: 0.8391\n",
      "Epoch 200/200\n",
      "3518/3518 [==============================] - 2s 535us/step - loss: 0.2593 - accuracy: 0.9076 - val_loss: 0.4542 - val_accuracy: 0.8356\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=200, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 1, ..., 6, 7, 3], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(x_testcnn)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 2, ..., 6, 7, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 2, ..., 6, 7, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Ytest = y_test.astype(int)\n",
    "new_Ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86       190\n",
      "           1       0.78      0.65      0.71       113\n",
      "           2       0.88      0.83      0.86       241\n",
      "           3       0.85      0.82      0.83       274\n",
      "           4       0.88      0.89      0.88       257\n",
      "           5       0.77      0.87      0.82       242\n",
      "           6       0.86      0.80      0.83       215\n",
      "           7       0.84      0.81      0.82       202\n",
      "\n",
      "    accuracy                           0.84      1734\n",
      "   macro avg       0.83      0.82      0.83      1734\n",
      "weighted avg       0.84      0.84      0.83      1734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(new_Ytest, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[175   4   0   9   0   2   0   0]\n",
      " [ 13  73   9  10   1   4   2   1]\n",
      " [  2   8 201   1  11   8   4   6]\n",
      " [  8   4   3 225   4  20   3   7]\n",
      " [  1   1   5   2 229  10   7   2]\n",
      " [  5   2   3  10   1 211   5   5]\n",
      " [  5   1   2   7   9   9 172  10]\n",
      " [  8   1   5   2   6   9   8 163]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(new_Ytest, predictions)\n",
    "print (matrix)\n",
    "\n",
    "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL SAVED\n"
     ]
    }
   ],
   "source": [
    "model.save('cnn/testing10_model.h5')\n",
    "print(\"MODEL SAVED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 40, 64)            384       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 40, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 10, 128)           41088     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 2, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 4104      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 209,672\n",
      "Trainable params: 209,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_model=keras.models.load_model('cnn/testing10_model.h5')\n",
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1734/1734 [==============================] - 0s 96us/step\n",
      "Restored model, accuracy: 83.56%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = new_model.evaluate(x_testcnn, y_test)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python374jvsc74a57bd002084d7c81d10e80320ab1bbc6c13aa20ecf0b140493bc380d73626ba95af753",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}